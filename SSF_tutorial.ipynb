{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2850ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os.path\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_undirected, from_scipy_sparse_matrix, dense_to_sparse, is_undirected, coalesce\n",
    "from torch_geometric.utils import contains_isolated_nodes, degree, remove_self_loops, k_hop_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730daff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28438466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a048e",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cda087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='email-Enron', data_split='0', val_ratio=0.2, seed=1, lr=0.0001, weight_decay=0.0005, walk_len=6, num_hops=2, hidden_channels=32, batch_size=32, epoch_num=1500, log=None)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Hyperlink Prediction by Extracting Subgraph Structural Features')\n",
    "parser.add_argument('--data', type=str,  default='email-Enron', help='graph name')\n",
    "parser.add_argument('--data_split',type=str, default='0', help='5 splits')\n",
    "#training/validation/test divison and ratio\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2,\n",
    "                    help='ratio of the validation set')\n",
    "#Model and Training\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "parser.add_argument('--walk_len', type=int, default=6, help='cutoff in the length of walks')\n",
    "parser.add_argument('--num_hops', type=int, default=2)\n",
    "parser.add_argument('--hidden_channels', type=int, default=32)\n",
    "parser.add_argument('--batch_size', type=int, default = 32)\n",
    "parser.add_argument('--epoch_num', type=int, default= 1500)\n",
    "\n",
    "parser.add_argument('--log', type=str, default=None,\n",
    "                    help='log by tensorboard, default is None')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a2e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_a_p(data):\n",
    "    \"\"\"\n",
    "    this function is used to obtain the adjacent matrix of the clique/line expansion graph.\n",
    "    # we tried to use the line graph to learn the feature of focal hyperedge, but the results are not good.\n",
    "    # so data. _p are actually not used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the adjacency matrix\n",
    "    index = torch.cat((data.node_idx, data.edge_idx), dim=0)\n",
    "    value = torch.ones_like(data.edge_idx).view(-1).to(torch.float)\n",
    "\n",
    "    i = torch.sparse_coo_tensor(index, value, (data.node_num, data.edge_num))\n",
    "    a = torch.sparse.mm(i, i.t())\n",
    "    a = a.coalesce()\n",
    "    data.edge_index_a, data.edge_weight_a = a.indices().to(torch.long), a.values().to(torch.float)\n",
    "    data.edge_index_a, data.edge_weight_a = remove_self_loops(data.edge_index_a, data.edge_weight_a)\n",
    "\n",
    "    # the intersection profile, i.e., the line graph's adjacent matrix\n",
    "    p = torch.sparse.mm(i.t(), i)\n",
    "    p = p.coalesce()\n",
    "    data.edge_index_p, data.edge_weight_p = p.indices().to(torch.long), p.values().to(torch.float)\n",
    "    data.edge_index_p, data.edge_weight_p = remove_self_loops(data.edge_index_p, data.edge_weight_p)\n",
    "    data.it = i.t()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44f44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_normalized_adjacency(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    row_sum = (row_sum == 0) * 1 + row_sum\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f72309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_walk_profile(data, args):\n",
    "    \"\"\"\n",
    "    for small graphs, using cpu is faster\n",
    "    for large graphs, using gpu is faster\n",
    "    \n",
    "    by default, we consider two cases in our edge weakening process.\n",
    "    \\alpha_set = {0,1}\n",
    "    \"\"\"\n",
    "    \n",
    "    walk_profile = list()\n",
    "    row, col = data.edge_index\n",
    "    row, col = row.view(-1), col.view(-1)\n",
    "    if data.edge_index.size(1) == 0:\n",
    "        return torch.zeros(1, args.walk_len * 10)\n",
    "\n",
    "    g = sp.csr_matrix((data.edge_weight.view(-1), (row, col)), shape=(data.num_nodes, data.num_nodes))\n",
    "    mat_p = sys_normalized_adjacency(g)\n",
    "        \n",
    "    mat_p = sparse_mx_to_torch_sparse_tensor(mat_p)\n",
    "    x_p = torch.eye(data.num_nodes, dtype=torch.float32)\n",
    "    # mat_p = sparse_mx_to_torch_sparse_tensor(mat_p).cuda()\n",
    "    # x_p = torch.eye(data.num_nodes, dtype=torch.float32).cuda()\n",
    "\n",
    "\n",
    "    # for adjacency matrix, the edge weight substract by one\n",
    "    edge_weight = data.edge_weight\n",
    "    edge_weight[data.edge_mask] -= 1.0\n",
    "\n",
    "    g1 = sp.csr_matrix((edge_weight.view(-1), (row, col)), shape=(data.num_nodes, data.num_nodes))\n",
    "    mat_m1 = sys_normalized_adjacency(g1)\n",
    "\n",
    "\n",
    "    mat_m1 = sparse_mx_to_torch_sparse_tensor(mat_m1)\n",
    "    x_m1 = torch.eye(data.num_nodes, dtype=torch.float32)\n",
    "    # mat_m1 = sparse_mx_to_torch_sparse_tensor(mat_m1).cuda()\n",
    "    # x_m1 = torch.eye(data.num_nodes, dtype=torch.float32).cuda()\n",
    "\n",
    "    # for adjacency matrix, set edge weights to zero\n",
    "    edge_weight = data.edge_weight\n",
    "    edge_weight[data.edge_mask] = 0\n",
    "\n",
    "    g2 = sp.csr_matrix((edge_weight.view(-1), (row, col)), shape=(data.num_nodes, data.num_nodes))\n",
    "    mat_m2 = sys_normalized_adjacency(g2)\n",
    "    mat_m2 = sparse_mx_to_torch_sparse_tensor(mat_m2)\n",
    "    x_m2 = torch.eye(data.num_nodes, dtype=torch.float32)\n",
    "    # mat_m2 = sparse_mx_to_torch_sparse_tensor(mat_m2).cuda()\n",
    "    # x_m2 = torch.eye(data.num_nodes, dtype=torch.float32).cuda()\n",
    "\n",
    "    for i in range(args.walk_len):\n",
    "        x_p = torch.spmm(mat_p, x_p)\n",
    "        x_m1 = torch.spmm(mat_m1, x_m1)\n",
    "        x_m2 = torch.spmm(mat_m2, x_m2)\n",
    "\n",
    "        walk_profile.append(torch.diagonal(x_p)[data.node_mask].mean())\n",
    "        walk_profile.append(x_p[data.node_mask, :][:, data.node_mask].mean())\n",
    "        walk_profile.append(torch.diagonal(x_m1)[data.node_mask].mean())\n",
    "        walk_profile.append(x_m1[data.node_mask, :][:, data.node_mask].mean())\n",
    "        walk_profile.append(torch.diagonal(x_p).mean() - torch.diagonal(x_m1).mean())\n",
    "        walk_profile.append(torch.diagonal(x_m2)[data.node_mask].mean())\n",
    "        walk_profile.append(x_m2[data.node_mask, :][:, data.node_mask].mean())\n",
    "        walk_profile.append(torch.diagonal(x_p).mean() - torch.diagonal(x_m2).mean())\n",
    "\n",
    "    walk_profile = torch.tensor(walk_profile, dtype=torch.float32).view(1, -1)\n",
    "    return walk_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e60bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_graph_minus(data, edge, h, args):\n",
    "    # the ego-graph in the adjaceny matarix\n",
    "    num_nodes_a = torch.max(data.edge_index_a) + 1\n",
    "    sub_nodes, edge_index_a, mapping, edge_mask = k_hop_subgraph(edge.view(-1), args.num_hops, data.edge_index_a,\n",
    "                                                                 relabel_nodes=True, num_nodes=num_nodes_a)\n",
    "    num_nodes_a = torch.max(edge_index_a) + 1\n",
    "    node_mask_a = torch.zeros(num_nodes_a, dtype=torch.bool)\n",
    "\n",
    "    # mask of the nodes contained in a hyperedge\n",
    "    node_mask_a[mapping] = True\n",
    "    edge_weight_a = data.edge_weight_a[edge_mask].view(-1, 1)\n",
    "\n",
    "    # mask of the edges run among the nodes\n",
    "    edge_mask_row = torch.zeros(edge_index_a.size(1), dtype=torch.bool)\n",
    "    edge_mask_col = torch.zeros(edge_index_a.size(1), dtype=torch.bool)\n",
    "\n",
    "    row, col = edge_index_a\n",
    "    torch.index_select(node_mask_a, 0, row, out=edge_mask_row)\n",
    "    torch.index_select(node_mask_a, 0, col, out=edge_mask_col)\n",
    "    edge_mask_a = torch.logical_and(edge_mask_row, edge_mask_col)\n",
    "\n",
    "    data_a = Data(edge_index=edge_index_a, edge_attr=edge_weight_a, num_nodes=num_nodes_a)\n",
    "    data_a.edge_weight = edge_weight_a\n",
    "    data_a.edge_mask = edge_mask_a\n",
    "    data_a.node_mask = node_mask_a\n",
    "\n",
    "    return data_a\n",
    "\n",
    "\n",
    "def ego_graph_plus(data, edge, args):\n",
    "    # add a hyperedge in the adjacency matrix\n",
    "    node_paris = torch.combinations(edge.view(-1)).transpose(0, 1)\n",
    "    edge_index_a = torch.cat((data.edge_index_a, node_paris), dim=1)\n",
    "    edge_index_a = torch.cat((edge_index_a, node_paris[[1, 0], :]), dim=1)\n",
    "    edge_weight_a = torch.cat((data.edge_weight_a, torch.ones(node_paris.size(1) * 2)), dim=0)\n",
    "    edge_index_a, edge_weight_a = coalesce(edge_index_a, edge_weight_a)\n",
    "\n",
    "    num_nodes_a = torch.max(edge_index_a) + 1\n",
    "    sub_nodes, edge_index_a, mapping, edge_mask = k_hop_subgraph(edge.view(-1), args.num_hops, edge_index_a,\n",
    "                                                                 relabel_nodes=True, num_nodes=num_nodes_a)\n",
    "    num_nodes_a = torch.max(edge_index_a) + 1\n",
    "    node_mask_a = torch.zeros(num_nodes_a, dtype=torch.bool)\n",
    "    node_mask_a[mapping] = True\n",
    "    edge_mask_row = torch.zeros(edge_index_a.size(1), dtype=torch.bool)\n",
    "    edge_mask_col = torch.zeros(edge_index_a.size(1), dtype=torch.bool)\n",
    "\n",
    "    row, col = edge_index_a\n",
    "    torch.index_select(node_mask_a, 0, row, out=edge_mask_row)\n",
    "    torch.index_select(node_mask_a, 0, col, out=edge_mask_col)\n",
    "    edge_mask_a = torch.logical_and(edge_mask_row, edge_mask_col)  # node pairs run inside the hyper edge\n",
    "    edge_weight_a = edge_weight_a[edge_mask].view(-1, 1)\n",
    "\n",
    "    data_a = Data(edge_index=edge_index_a, edge_attr=edge_weight_a, num_nodes=num_nodes_a)\n",
    "    data_a.edge_weight = edge_weight_a\n",
    "    data_a.edge_mask = edge_mask_a\n",
    "    data_a.node_mask = node_mask_a\n",
    "\n",
    "    return data_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cb65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splitted_data(args):\n",
    "    par_dir = os.path.abspath('')\n",
    "    data_name = args.data + \"_split_\" + args.data_split\n",
    "    data_dir = os.path.join(par_dir, \"data/{}.npz\".format(data_name))\n",
    "    data = np.load(data_dir, allow_pickle=True)\n",
    "    train_data = data['arr_0']\n",
    "    train_label = data['arr_1']\n",
    "    test_data = data['arr_2']\n",
    "    test_label = data['arr_3']\n",
    "\n",
    "    # Convert to Torch tensors\n",
    "    train_label = torch.tensor(train_label, dtype=torch.long)\n",
    "    test_lb = torch.tensor(test_label, dtype=torch.long)\n",
    "\n",
    "    pos_ind = torch.where(train_label == 1)[0]\n",
    "    neg_ind = torch.where(train_label == 0)[0]\n",
    "    pos_val_size = int(pos_ind.size(0) * args.val_ratio)\n",
    "    neg_val_size = int(neg_ind.size(0) * args.val_ratio)\n",
    "    is_train = torch.ones_like(train_label).to(torch.bool)\n",
    "    perm = torch.randperm(pos_ind.size(0))\n",
    "    is_train[pos_ind[perm[:pos_val_size]]] = False\n",
    "    perm = torch.randperm(neg_ind.size(0))\n",
    "    is_train[neg_ind[perm[:neg_val_size]]] = False\n",
    "\n",
    "    train_edges = list()\n",
    "    val_edges = list()\n",
    "    test_edges = list()\n",
    "    train_lb = list()\n",
    "    val_lb = list()\n",
    "    train_pos_id = list()\n",
    "    val_pos_id = list()\n",
    "\n",
    "    # contruct the observed hypergraph\n",
    "    edge_id = -1\n",
    "    num_nodes = 0\n",
    "    node_idx, edge_idx = torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n",
    "    for ind, edge in enumerate(train_data):\n",
    "        nodes = torch.tensor(edge).view(1, -1).to(torch.long)\n",
    "        max_node = torch.max(nodes)\n",
    "        if max_node > num_nodes:\n",
    "            num_nodes = max_node\n",
    "        if is_train[ind]:\n",
    "            train_edges.append(nodes)\n",
    "            train_lb.append(train_label[ind])\n",
    "            if train_label[ind] == 1:\n",
    "                edge_id += 1\n",
    "                edges = torch.full(nodes.size(), edge_id).to(torch.long)\n",
    "                node_idx = torch.cat((node_idx, nodes), dim=1)\n",
    "                edge_idx = torch.cat((edge_idx, edges), dim=1)\n",
    "                train_pos_id.append(edge_id)\n",
    "        else:\n",
    "            val_edges.append(nodes)\n",
    "            val_lb.append(train_label[ind])\n",
    "            if train_label[ind] == 1:\n",
    "                edge_id += 1\n",
    "                edges = torch.full(nodes.size(), edge_id).to(torch.long)\n",
    "                node_idx = torch.cat((node_idx, nodes), dim=1)\n",
    "                edge_idx = torch.cat((edge_idx, edges), dim=1)\n",
    "                val_pos_id.append(edge_id)\n",
    "\n",
    "    for ind, edge in enumerate(test_data):\n",
    "        max_node = torch.max(nodes)\n",
    "        if max_node > num_nodes:\n",
    "            num_nodes = max_node\n",
    "        nodes = torch.tensor(edge).view(1, -1).to(torch.long)\n",
    "        test_edges.append(nodes)\n",
    "\n",
    "    data = Data()\n",
    "    data.node_idx = node_idx\n",
    "    data.edge_idx = edge_idx\n",
    "    data.node_num = int(num_nodes + 1)\n",
    "    data.edge_num = int(edge_id + 1)\n",
    "    data = obtain_a_p(data)\n",
    "\n",
    "    train_lb = torch.tensor(train_lb).to(torch.long)\n",
    "    val_lb = torch.tensor(val_lb).to(torch.long)\n",
    "\n",
    "    return data, train_edges, val_edges, test_edges, train_lb, val_lb, test_lb, train_pos_id, val_pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a1f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Loader and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20beab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1988/1988 [00:21<00:00, 91.09it/s]\n",
      "100%|█████████████████████████████████████████| 496/496 [00:05<00:00, 89.42it/s]\n",
      "100%|█████████████████████████████████████████| 408/408 [00:04<00:00, 87.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Complete generating training data>>\n",
      "num_train_edges:  1988\n",
      "num_val_edges:  496\n",
      "num_test_edges:  408\n",
      "------------------------------------------Model and Training---------------------------------------------\n",
      "Learning Rate|Weight Decay |Batch Size   |Epoch   |Walk Length  |Hidden Channels\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "0.0001       |0.0005       |32           |1500    |6            |32             \n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data, train_edges, val_edges, test_edges, train_lb, val_lb, test_lb, train_pos_ids, val_pos_ids = load_splitted_data(args)\n",
    "\"\"\"\n",
    "we strictly divide train/val/test, only train hyperedges are used in the construction of observed hypergraph. \n",
    "given an abtrary hyperedge, we extract features based on this hyperedge and the observed hypergraph.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "set_random_seed(args.seed)\n",
    "train_data = torch.tensor([])\n",
    "val_data = torch.tensor([])\n",
    "test_data = torch.tensor([])\n",
    "\n",
    "train_pos_id = -1\n",
    "train_labels = []\n",
    "for ind, edge in enumerate(tqdm(train_edges)):\n",
    "    if edge.shape[1] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        if train_lb[ind] == 1:\n",
    "            train_labels.append(1)\n",
    "            train_pos_id = train_pos_ids.pop(0)\n",
    "            data_a = ego_graph_minus(data, edge, train_pos_id, args)\n",
    "        else:\n",
    "            train_labels.append(0)\n",
    "            data_a = ego_graph_plus(data, edge, args)\n",
    "        walk_profile = obtain_walk_profile(data_a, args)\n",
    "        train_data = torch.cat((train_data, walk_profile), dim=0)\n",
    "\n",
    "val_labels = []\n",
    "for ind, edge in enumerate(tqdm(val_edges)):\n",
    "    if edge.shape[1] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        if val_lb[ind] == 1:\n",
    "            val_labels.append(1)\n",
    "            val_pos_id = val_pos_ids.pop(0)\n",
    "            data_a = ego_graph_minus(data, edge, val_pos_id, args)\n",
    "        else:\n",
    "            val_labels.append(0)\n",
    "            data_a = ego_graph_plus(data, edge, args)\n",
    "        walk_profile = obtain_walk_profile(data_a, args)\n",
    "        val_data = torch.cat((val_data, walk_profile), dim=0)\n",
    "\n",
    "test_labels = []\n",
    "for ind, edge in enumerate(tqdm(test_edges)):\n",
    "    if edge.shape[1] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        test_labels.append(test_lb[ind])\n",
    "        data_a = ego_graph_plus(data, edge, args)\n",
    "        walk_profile = obtain_walk_profile(data_a, args)\n",
    "        test_data = torch.cat((test_data, walk_profile), dim=0)\n",
    "\n",
    "train_lb = torch.tensor(train_labels).to(torch.long)\n",
    "val_lb = torch.tensor(val_labels).to(torch.long)\n",
    "test_lb = torch.tensor(test_labels).to(torch.long)\n",
    "\n",
    "print('<<Complete generating training data>>')\n",
    "\n",
    "print(\"num_train_edges: \", len(train_edges))\n",
    "print(\"num_val_edges: \", len(val_edges))\n",
    "print(\"num_test_edges: \", len(test_edges))\n",
    "\n",
    "print (\"-\"*42+'Model and Training'+\"-\"*45)\n",
    "print (\"{:<13}|{:<13}|{:<13}|{:<8}|{:<13}|{:<15}\"\\\n",
    "    .format('Learning Rate','Weight Decay','Batch Size','Epoch',\\\n",
    "        'Walk Length','Hidden Channels'))\n",
    "print (\"-\"*105)\n",
    "\n",
    "print (\"{:<13}|{:<13}|{:<13}|{:<8}|{:<13}|{:<15}\"\\\n",
    "    .format(args.lr,args.weight_decay, str(args.batch_size),\\\n",
    "        args.epoch_num,args.walk_len, args.hidden_channels))\n",
    "print (\"-\"*105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8d6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    # adopt a MLP as classifier for graphs\n",
    "    def __init__(self,input_size, hidden_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        self.nn = nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size,hidden_channels*20)\n",
    "        self.linear2 = torch.nn.Linear(hidden_channels*20,hidden_channels*10)\n",
    "        self.linear3 = torch.nn.Linear(hidden_channels*10,hidden_channels)\n",
    "        self.linear4 = torch.nn.Linear(hidden_channels,hidden_channels)\n",
    "        self.linear5 = torch.nn.Linear(hidden_channels,1)\n",
    "        self.act= nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.nn(x)\n",
    "        out= self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out= self.linear2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.act(out)\n",
    "        out = F.dropout(out, p=0.7, training=self.training)\n",
    "        out = self.linear5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2be0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, lbs):\n",
    "    classifier.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    out = classifier(data)\n",
    "    loss = criterion(out.view(-1), lbs.view(-1).to(torch.float))\n",
    "    optimizer_classifier.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_classifier.step()\n",
    "    loss_epoch = loss.item()\n",
    "    return loss_epoch\n",
    "\n",
    "def test(data, lbs):\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        out = classifier(data)\n",
    "        loss = criterion(out.view(-1), lbs.view(-1).to(torch.float))\n",
    "        scores = out.cpu().clone().detach()\n",
    "        tpred = scores.flatten()\n",
    "        num_predictions = int(sum(lbs))\n",
    "        cut = np.partition(tpred, -num_predictions)[-num_predictions]\n",
    "        tpred = torch.Tensor(tpred)\n",
    "        pred = torch.where(tpred >= cut, torch.ones_like(tpred), torch.zeros_like(tpred)).view(-1)\n",
    "        # pred = torch.sigmoid(scores)\n",
    "        # pred = torch.where(pred>0.6, torch.ones_like(pred), torch.zeros_like(pred)).view(-1)\n",
    "        f1 = f1_score(np.array(lbs.cpu().tolist()), np.array(pred.cpu().tolist()))\n",
    "        auc = roc_auc_score(np.array(lbs.cpu().tolist()), scores)\n",
    "        ap = average_precision_score(np.array(lbs.cpu().tolist()), scores)\n",
    "        return f1, auc, ap, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd5871b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of features after concatenation:  48\n"
     ]
    }
   ],
   "source": [
    "walk_len = args.walk_len\n",
    "hidden_channels=args.hidden_channels\n",
    "lr=args.lr\n",
    "weight_decay=args.weight_decay\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "args.num_features = train_data.shape[1]\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Dimention of features after concatenation: \", args.num_features)\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "\n",
    "classifier = MLP(train_data.size(1), args.hidden_channels).to(device)\n",
    "optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.0]).to(device))\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "train_lb = train_lb.to(device)\n",
    "val_lb = val_lb.to(device)\n",
    "test_lb = test_lb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "563c8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss : 1.1646, Test AUC: 0.6415, Test AP: 0.2511, Test f1: 0.2647, Picked f1: 0.2647\n",
      "Epoch: 050, Loss : 1.0621, Test AUC: 0.8122, Test AP: 0.5271, Test f1: 0.5735, Picked f1: 0.5735\n",
      "Epoch: 100, Loss : 0.8917, Test AUC: 0.8269, Test AP: 0.6037, Test f1: 0.6176, Picked f1: 0.6176\n",
      "Epoch: 150, Loss : 0.8275, Test AUC: 0.8399, Test AP: 0.6317, Test f1: 0.6176, Picked f1: 0.6176\n",
      "Epoch: 200, Loss : 0.7477, Test AUC: 0.8639, Test AP: 0.6601, Test f1: 0.6324, Picked f1: 0.6176\n",
      "Epoch: 250, Loss : 0.7177, Test AUC: 0.8823, Test AP: 0.7100, Test f1: 0.6324, Picked f1: 0.6324\n",
      "Epoch: 300, Loss : 0.6937, Test AUC: 0.8859, Test AP: 0.7224, Test f1: 0.6176, Picked f1: 0.6324\n",
      "Epoch: 350, Loss : 0.6443, Test AUC: 0.8892, Test AP: 0.7388, Test f1: 0.6471, Picked f1: 0.6176\n",
      "Epoch: 400, Loss : 0.6296, Test AUC: 0.8917, Test AP: 0.7521, Test f1: 0.6618, Picked f1: 0.6176\n",
      "Epoch: 450, Loss : 0.5843, Test AUC: 0.8980, Test AP: 0.7691, Test f1: 0.6618, Picked f1: 0.6176\n",
      "Epoch: 500, Loss : 0.5869, Test AUC: 0.8974, Test AP: 0.7727, Test f1: 0.7059, Picked f1: 0.6176\n",
      "Epoch: 550, Loss : 0.5639, Test AUC: 0.8972, Test AP: 0.7735, Test f1: 0.7059, Picked f1: 0.6912\n",
      "Epoch: 600, Loss : 0.5339, Test AUC: 0.8974, Test AP: 0.7774, Test f1: 0.6912, Picked f1: 0.6912\n",
      "Epoch: 650, Loss : 0.5208, Test AUC: 0.8983, Test AP: 0.7774, Test f1: 0.7059, Picked f1: 0.6912\n",
      "Epoch: 700, Loss : 0.5158, Test AUC: 0.8957, Test AP: 0.7747, Test f1: 0.7059, Picked f1: 0.6912\n",
      "Epoch: 750, Loss : 0.4944, Test AUC: 0.8974, Test AP: 0.7754, Test f1: 0.6912, Picked f1: 0.6912\n",
      "Epoch: 800, Loss : 0.4960, Test AUC: 0.8972, Test AP: 0.7716, Test f1: 0.6912, Picked f1: 0.6912\n",
      "Epoch: 850, Loss : 0.4508, Test AUC: 0.9003, Test AP: 0.7709, Test f1: 0.7059, Picked f1: 0.6912\n",
      "Epoch: 900, Loss : 0.4463, Test AUC: 0.8995, Test AP: 0.7635, Test f1: 0.6765, Picked f1: 0.6912\n",
      "Epoch: 950, Loss : 0.4244, Test AUC: 0.8967, Test AP: 0.7609, Test f1: 0.6765, Picked f1: 0.6912\n",
      "Epoch: 1000, Loss : 0.4131, Test AUC: 0.8937, Test AP: 0.7502, Test f1: 0.6765, Picked f1: 0.6912\n",
      "Epoch: 1050, Loss : 0.3820, Test AUC: 0.8921, Test AP: 0.7475, Test f1: 0.6765, Picked f1: 0.6912\n",
      "Epoch: 1100, Loss : 0.3750, Test AUC: 0.8864, Test AP: 0.7386, Test f1: 0.6765, Picked f1: 0.6912\n",
      "Epoch: 1150, Loss : 0.3413, Test AUC: 0.8838, Test AP: 0.7204, Test f1: 0.6471, Picked f1: 0.6912\n",
      "Epoch: 1200, Loss : 0.3377, Test AUC: 0.8831, Test AP: 0.7247, Test f1: 0.6471, Picked f1: 0.6912\n",
      "Epoch: 1250, Loss : 0.3099, Test AUC: 0.8771, Test AP: 0.6969, Test f1: 0.6029, Picked f1: 0.6912\n",
      "Epoch: 1300, Loss : 0.3024, Test AUC: 0.8760, Test AP: 0.7034, Test f1: 0.6471, Picked f1: 0.6912\n",
      "Epoch: 1350, Loss : 0.2974, Test AUC: 0.8773, Test AP: 0.7099, Test f1: 0.6324, Picked f1: 0.6912\n",
      "Epoch: 1400, Loss : 0.2882, Test AUC: 0.8784, Test AP: 0.7059, Test f1: 0.6324, Picked f1: 0.6912\n",
      "Epoch: 1450, Loss : 0.2751, Test AUC: 0.8753, Test AP: 0.6985, Test f1: 0.6324, Picked f1: 0.6912\n"
     ]
    }
   ],
   "source": [
    "best_from_val = 0\n",
    "best_val_f1 = 0\n",
    "\n",
    "\"\"\"\n",
    "we use full-epoch training.\n",
    "during the test phase, we assume the number of predictions are given.\n",
    "this assumption is generally used in (hyper-)link prediction studies.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for epoch in range(0, args.epoch_num):\n",
    "    train_loss = train(train_data, train_lb)\n",
    "    frac = sum(train_lb)/len(train_lb)\n",
    "    val_f1, val_auc, val_ap, val_loss = test(val_data, val_lb)\n",
    "    test_f1, test_auc, test_ap, test_loss = test(test_data, test_lb)\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_from_val = test_f1\n",
    "        best_auc = test_auc\n",
    "        best_ap = test_ap\n",
    "    if epoch % 50 ==0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss : {train_loss:.4f}, Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}, Test f1: {test_f1:.4f}, Picked f1: {best_from_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1769017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
